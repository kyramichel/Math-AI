{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Mathematics Underlying Neural Network Model Creation and Training\n",
        "\n",
        "**Tensors** are the fundamental data structures of neural networks.\n",
        "\n",
        "This notebook provides a detailed exploration of the mathematical operations and concepts that underlie neural network model building. For simplicity, the Iris dataset is used alongside TensorFlow to illustrate the operations involving tensors in the model architecture, with a particular focus on the first dense layer, activation functions, and batch normalization.\n",
        "\n",
        "The notebook emphasizes the importance of understanding tensor operations when building neural networks.\n"
      ],
      "metadata": {
        "id": "2JNM6gfZWKEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Python Code:\n",
        "\n",
        "Building and Training a Neural Network Model with TensorFlow to Classify the Iris Dataset\n"
      ],
      "metadata": {
        "id": "aVnvrMtyY_KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')  # 3 output neurons for the 3 classes in the Iris dataset\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=8, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "\n",
        "# Evaluate the model using accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEe4VzlBYtkR",
        "outputId": "95425b80-0e9c-4bcf-d120-a01b1a93aabb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4314 - loss: 1.7515 - val_accuracy: 0.6667 - val_loss: 0.9488\n",
            "Epoch 2/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6000 - loss: 0.8342 - val_accuracy: 0.7083 - val_loss: 0.8571\n",
            "Epoch 3/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6171 - loss: 1.0188 - val_accuracy: 0.7083 - val_loss: 0.7894\n",
            "Epoch 4/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7471 - loss: 0.7989 - val_accuracy: 0.7083 - val_loss: 0.7263\n",
            "Epoch 5/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6785 - loss: 0.8814 - val_accuracy: 0.7500 - val_loss: 0.6753\n",
            "Epoch 6/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6954 - loss: 0.8061 - val_accuracy: 0.7917 - val_loss: 0.6338\n",
            "Epoch 7/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7590 - loss: 0.4915 - val_accuracy: 0.8333 - val_loss: 0.5931\n",
            "Epoch 8/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8177 - loss: 0.5225 - val_accuracy: 0.8750 - val_loss: 0.5544\n",
            "Epoch 9/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7320 - loss: 0.7222 - val_accuracy: 0.9167 - val_loss: 0.5189\n",
            "Epoch 10/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7930 - loss: 0.5758 - val_accuracy: 0.9167 - val_loss: 0.4921\n",
            "Epoch 11/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7792 - loss: 0.6092 - val_accuracy: 0.9167 - val_loss: 0.4532\n",
            "Epoch 12/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7576 - loss: 0.5682 - val_accuracy: 0.8750 - val_loss: 0.4213\n",
            "Epoch 13/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7234 - loss: 0.6928 - val_accuracy: 0.8750 - val_loss: 0.3908\n",
            "Epoch 14/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8050 - loss: 0.4339 - val_accuracy: 0.8750 - val_loss: 0.3614\n",
            "Epoch 15/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7990 - loss: 0.4755 - val_accuracy: 0.8750 - val_loss: 0.3384\n",
            "Epoch 16/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8682 - loss: 0.3646 - val_accuracy: 0.8750 - val_loss: 0.3169\n",
            "Epoch 17/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7733 - loss: 0.6008 - val_accuracy: 0.9167 - val_loss: 0.3015\n",
            "Epoch 18/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8537 - loss: 0.3341 - val_accuracy: 0.9583 - val_loss: 0.2880\n",
            "Epoch 19/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9171 - loss: 0.2691 - val_accuracy: 0.9167 - val_loss: 0.2796\n",
            "Epoch 20/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8271 - loss: 0.4676 - val_accuracy: 0.9167 - val_loss: 0.2697\n",
            "Epoch 21/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8797 - loss: 0.3923 - val_accuracy: 0.9167 - val_loss: 0.2514\n",
            "Epoch 22/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8454 - loss: 0.4294 - val_accuracy: 0.9583 - val_loss: 0.2386\n",
            "Epoch 23/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8770 - loss: 0.2938 - val_accuracy: 0.9167 - val_loss: 0.2319\n",
            "Epoch 24/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8872 - loss: 0.3276 - val_accuracy: 0.9167 - val_loss: 0.2264\n",
            "Epoch 25/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8022 - loss: 0.4896 - val_accuracy: 0.9583 - val_loss: 0.2152\n",
            "Epoch 26/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8321 - loss: 0.4452 - val_accuracy: 0.9167 - val_loss: 0.2195\n",
            "Epoch 27/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9041 - loss: 0.2778 - val_accuracy: 0.9167 - val_loss: 0.2136\n",
            "Epoch 28/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9539 - loss: 0.1857 - val_accuracy: 0.9167 - val_loss: 0.2160\n",
            "Epoch 29/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9074 - loss: 0.2560 - val_accuracy: 0.9167 - val_loss: 0.2167\n",
            "Epoch 30/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9147 - loss: 0.2896 - val_accuracy: 0.9167 - val_loss: 0.2176\n",
            "Epoch 31/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9226 - loss: 0.1647 - val_accuracy: 0.9167 - val_loss: 0.2144\n",
            "Epoch 32/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8825 - loss: 0.2738 - val_accuracy: 0.9167 - val_loss: 0.2113\n",
            "Epoch 33/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8074 - loss: 0.4476 - val_accuracy: 0.9167 - val_loss: 0.2033\n",
            "Epoch 34/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8920 - loss: 0.3330 - val_accuracy: 0.9167 - val_loss: 0.2080\n",
            "Epoch 35/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9292 - loss: 0.2657 - val_accuracy: 0.9583 - val_loss: 0.1881\n",
            "Epoch 36/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9607 - loss: 0.1685 - val_accuracy: 0.9583 - val_loss: 0.1783\n",
            "Epoch 37/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8604 - loss: 0.3946 - val_accuracy: 0.9583 - val_loss: 0.1692\n",
            "Epoch 38/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9218 - loss: 0.2778 - val_accuracy: 0.9583 - val_loss: 0.1664\n",
            "Epoch 39/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9044 - loss: 0.2058 - val_accuracy: 0.9583 - val_loss: 0.1613\n",
            "Epoch 40/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8623 - loss: 0.3681 - val_accuracy: 0.9583 - val_loss: 0.1595\n",
            "Epoch 41/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8993 - loss: 0.2501 - val_accuracy: 0.9583 - val_loss: 0.1590\n",
            "Epoch 42/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8497 - loss: 0.4016 - val_accuracy: 0.9583 - val_loss: 0.1699\n",
            "Epoch 43/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8516 - loss: 0.2558 - val_accuracy: 0.9583 - val_loss: 0.1670\n",
            "Epoch 44/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8669 - loss: 0.3986 - val_accuracy: 0.9583 - val_loss: 0.1594\n",
            "Epoch 45/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9417 - loss: 0.2191 - val_accuracy: 0.9583 - val_loss: 0.1559\n",
            "Epoch 46/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9155 - loss: 0.1765 - val_accuracy: 0.9583 - val_loss: 0.1502\n",
            "Epoch 47/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8259 - loss: 0.3698 - val_accuracy: 0.9583 - val_loss: 0.1555\n",
            "Epoch 48/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8765 - loss: 0.3291 - val_accuracy: 0.9583 - val_loss: 0.1570\n",
            "Epoch 49/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9740 - loss: 0.1096 - val_accuracy: 0.9583 - val_loss: 0.1561\n",
            "Epoch 50/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9188 - loss: 0.2098 - val_accuracy: 0.9583 - val_loss: 0.1532\n",
            "Epoch 51/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8627 - loss: 0.3440 - val_accuracy: 0.9583 - val_loss: 0.1428\n",
            "Epoch 52/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8590 - loss: 0.3330 - val_accuracy: 0.9583 - val_loss: 0.1576\n",
            "Epoch 53/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8927 - loss: 0.2979 - val_accuracy: 0.9583 - val_loss: 0.1622\n",
            "Epoch 54/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8864 - loss: 0.3128 - val_accuracy: 0.9583 - val_loss: 0.1563\n",
            "Epoch 55/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9606 - loss: 0.1138 - val_accuracy: 0.9583 - val_loss: 0.1547\n",
            "Epoch 56/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9249 - loss: 0.1916 - val_accuracy: 0.9583 - val_loss: 0.1367\n",
            "Epoch 57/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9053 - loss: 0.2434 - val_accuracy: 0.9583 - val_loss: 0.1245\n",
            "Epoch 58/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9105 - loss: 0.1891 - val_accuracy: 0.9583 - val_loss: 0.1241\n",
            "Epoch 59/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8641 - loss: 0.5440 - val_accuracy: 0.9583 - val_loss: 0.1316\n",
            "Epoch 60/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9020 - loss: 0.4378 - val_accuracy: 0.9583 - val_loss: 0.1461\n",
            "Epoch 61/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9187 - loss: 0.2273 - val_accuracy: 0.9583 - val_loss: 0.1259\n",
            "Epoch 62/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8802 - loss: 0.2877 - val_accuracy: 0.9167 - val_loss: 0.1468\n",
            "Epoch 63/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8707 - loss: 0.2704 - val_accuracy: 0.9167 - val_loss: 0.1377\n",
            "Epoch 64/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9488 - loss: 0.1224 - val_accuracy: 0.9167 - val_loss: 0.1506\n",
            "Epoch 65/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9080 - loss: 0.2333 - val_accuracy: 0.9167 - val_loss: 0.1510\n",
            "Epoch 66/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9335 - loss: 0.1713 - val_accuracy: 0.9167 - val_loss: 0.1405\n",
            "Epoch 67/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9230 - loss: 0.2099 - val_accuracy: 0.9583 - val_loss: 0.1202\n",
            "Epoch 68/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8834 - loss: 0.4581 - val_accuracy: 0.9583 - val_loss: 0.1110\n",
            "Epoch 69/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7894 - loss: 0.4181 - val_accuracy: 0.9583 - val_loss: 0.1007\n",
            "Epoch 70/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9330 - loss: 0.1802 - val_accuracy: 0.9583 - val_loss: 0.0937\n",
            "Epoch 71/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9189 - loss: 0.2276 - val_accuracy: 0.9583 - val_loss: 0.0869\n",
            "Epoch 72/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9449 - loss: 0.1767 - val_accuracy: 0.9583 - val_loss: 0.0929\n",
            "Epoch 73/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9645 - loss: 0.1757 - val_accuracy: 0.9583 - val_loss: 0.0970\n",
            "Epoch 74/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9038 - loss: 0.3108 - val_accuracy: 0.9583 - val_loss: 0.0923\n",
            "Epoch 75/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8707 - loss: 0.2576 - val_accuracy: 0.9583 - val_loss: 0.0859\n",
            "Epoch 76/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9692 - loss: 0.1004 - val_accuracy: 1.0000 - val_loss: 0.0870\n",
            "Epoch 77/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9626 - loss: 0.1764 - val_accuracy: 1.0000 - val_loss: 0.0922\n",
            "Epoch 78/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9607 - loss: 0.1257 - val_accuracy: 0.9583 - val_loss: 0.0956\n",
            "Epoch 79/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8871 - loss: 0.2488 - val_accuracy: 0.9167 - val_loss: 0.1097\n",
            "Epoch 80/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8410 - loss: 0.3751 - val_accuracy: 0.9583 - val_loss: 0.1051\n",
            "Epoch 81/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9431 - loss: 0.1468 - val_accuracy: 0.9167 - val_loss: 0.1155\n",
            "Epoch 82/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9550 - loss: 0.1459 - val_accuracy: 0.9167 - val_loss: 0.1322\n",
            "Epoch 83/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9210 - loss: 0.2649 - val_accuracy: 0.9167 - val_loss: 0.1116\n",
            "Epoch 84/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9155 - loss: 0.6796 - val_accuracy: 0.9583 - val_loss: 0.1029\n",
            "Epoch 85/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8992 - loss: 0.2445 - val_accuracy: 0.9167 - val_loss: 0.1105\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 581ms/step - accuracy: 0.9667 - loss: 0.0699\n",
            "Test Accuracy: 0.9666666388511658\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step\n",
            "Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "Iwpy5y-5wrQh",
        "outputId": "90775ac6-64b1-4675-a869-e9b3f2109004"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,051\u001b[0m (58.80 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,051</span> (58.80 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,931\u001b[0m (19.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,931</span> (19.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m256\u001b[0m (1.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m9,864\u001b[0m (38.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,864</span> (38.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Explanation of Model Architecture:\n",
        "\n",
        "- **Dense Layer**: Each neuron computes a weighted sum of inputs:\n",
        "  $$ z = W \\cdot X + b $$\n",
        "\n",
        "- **Activation Function (ReLU)**: Defined as:\n",
        "  $$ \\text{ReLU}(z) = \\max(0, z) $$\n",
        "\n",
        "- **Batch Normalization**: Normalizes outputs to enhance training speed and stability.\n",
        "\n",
        "- **Dropout**: Randomly sets a fraction of input units to 0 during training to mitigate overfitting.\n",
        "\n",
        "- **Output Layer**: Utilizes the softmax function for multi-class classification:\n",
        "  $$ P(y = k | X) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}} $$\n",
        "\n"
      ],
      "metadata": {
        "id": "lLm_Ps0z6XaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Mathematics of Model Architecture\n",
        "\n",
        "1. **Dense Layer**:\n",
        "   - **Input**: $ \\mathbf{X} $ of shape $(n, m)$, where $ n $ is the batch size and $ m $ is the number of input features.\n",
        "   - **Weights and Biases**: $ \\mathbf{W} $ of shape $(m, k)$ and $ \\mathbf{b} $ of shape $(k,)$.\n",
        "   - **Linear Transformation**:\n",
        "     $$\n",
        "     \\mathbf{Z} = \\mathbf{X} \\mathbf{W} + \\mathbf{b} $$\n",
        "   - **Activation Function** (e.g., ReLU):\n",
        "     $$\n",
        "     \\mathbf{A} = \\sigma(\\mathbf{Z}) $$\n",
        "   - **Output**: $ \\mathbf{A} $ of shape $(n, k)$.\n",
        "\n",
        "2. **Batch Normalization Layer**:\n",
        "   - **Input**: $ \\mathbf{A} $ of shape $(n, k)$.\n",
        "   - **Mean and Variance**:\n",
        "  $$\n",
        "  \\mu = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{A}_i $$\n",
        "\n",
        "     $$\n",
        "     \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{A}_i - \\mu)^2 $$\n",
        "   - **Normalization**:\n",
        "     $$\n",
        "     \\hat{\\mathbf{A}} = \\frac{\\mathbf{A} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $$\n",
        "   - **Scaling and Shifting**:\n",
        "     $$\n",
        "     \\mathbf{Y} = \\gamma \\hat{\\mathbf{A}} + \\beta $$\n",
        "   - **Output**: $ \\mathbf{Y} $ of shape $(n, k)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ukl1k5-NhBfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### The Output $Y$ from 1st Dense and Batch Normalization Layers:\n",
        "\n",
        "The output $Y$ provides a transformed representation of the input data $\\mathbf{X}_{\\text{train}}$, which can be further processed in subsequent layers of a neural network. Here's a more detailed explanation:\n",
        "\n",
        "### Transformed Representation\n",
        "\n",
        "1. **Dense Layer Output $\\mathbf{A}$**:\n",
        "   - The Dense Layer applies a linear transformation to the input data $\\mathbf{X}_{\\text{train}}$ using weights and biases, followed by an activation function (in this case, ReLU). This transformation allows the model to learn complex patterns and relationships in the data.\n",
        "\n",
        "2. **Batch Normalization Output $\\mathbf{Y}$**:\n",
        "   - The Batch Normalization Layer normalizes the output of the Dense Layer, which helps to mitigate issues related to internal covariate shift. By normalizing the activations, the model can learn more effectively, as the inputs to each layer remain more stable throughout training.\n",
        "\n",
        "### Benefits of Normalization\n",
        "\n",
        "- **Stabilizes Learning**: Normalization reduces the sensitivity of the network to the scale of the inputs, making the training process more stable.\n",
        "- **Faster Convergence**: By keeping the activations within a certain range, normalization can lead to faster convergence during training, allowing the model to reach optimal weights more quickly.\n",
        "- **Improved Performance**: Normalization can help improve the overall performance of the model, as it allows for better gradient flow and reduces the likelihood of vanishing or exploding gradients.\n",
        "\n",
        "### Subsequent Layers\n",
        "\n",
        "After the Batch Normalization Layer, the output $\\mathbf{Y}$ is fed into additional layers of the neural network, such as:\n",
        "\n",
        "- **Dropout Layers**: To prevent overfitting.\n",
        "- **Additional Dense Layers**: To further learn complex representations.\n",
        "- **Activation Layers**: To introduce non-linearity.\n",
        "- **Output Layers**: To produce the final predictions.\n"
      ],
      "metadata": {
        "id": "MIKoboBv7WO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The transformed representations from the Dense Layer and Batch Normalization Layer are crucial for the effective training and performance of neural networks, enabling them to learn from the input data $\\mathbf{X}_{\\text{train}}$ more efficiently.\n",
        "\n",
        "## Example:\n",
        "\n"
      ],
      "metadata": {
        "id": "tMWbdEszwEaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1. Dense Layer Calculation:\n",
        "\n",
        "### Step 1.Linear Transformation\n",
        "\n",
        "We need to compute $\\mathbf{Z} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}$:\n",
        "\n",
        "1. **Input $ \\mathbf{X} $**:\n",
        "   $\n",
        "   \\mathbf{X} = \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix}\n",
        "   $\n",
        "\n",
        "2. **Weights $ \\mathbf{W} $**:\n",
        "   $\n",
        "   \\mathbf{W} = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.5 & 0.6 & 0.7 & 0.8 \\\\ 0.9 & 1.0 & 1.1 & 1.2 \\end{bmatrix}\n",
        "   $\n",
        "\n",
        "3. **Biases $ \\mathbf{b} $**:\n",
        "   $\n",
        "   \\mathbf{b} = \\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ 0.4 \\end{bmatrix}\n",
        "   $\n",
        "\n",
        "#### Calculation of $ \\mathbf{Z} $:\n",
        "\n",
        "- First, we compute $ \\mathbf{X} \\mathbf{W} $:\n",
        "\n",
        "$ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.5 & 0.6 & 0.7 & 0.8 \\\\ 0.9 & 1.0 & 1.1 & 1.2 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "Calculating each element of the resulting matrix:\n",
        "\n",
        "- For the first row:\n",
        "  - First column: $ 1.0 \\cdot 0.1 + 2.0 \\cdot 0.5 + 3.0 \\cdot 0.9 = 0.1 + 1.0 + 2.7 = 3.8 $\n",
        "  - Second column: $ 1.0 \\cdot 0.2 + 2.0 \\cdot 0.6 + 3.0 \\cdot 1.0 = 0.2 + 1.2 + 3.0 = 4.4 $\n",
        "  - Third column: $ 1.0 \\cdot 0.3 + 2.0 \\cdot 0.7 + 3.0 \\cdot 1.1 = 0.3 + 1.4 + 3.3 = 5.0 $\n",
        "  - Fourth column: $ 1.0 \\cdot 0.4 + 2.0 \\cdot 0.8 + 3.0 \\cdot 1.2 = 0.4 + 1.6 + 3.6 = 5.6 $\n",
        "\n",
        "- For the second row:\n",
        "  - First column: $ 4.0 \\cdot 0.1 + 5.0 \\cdot 0.5 + 6.0 \\cdot 0.9 = 0.4 + 2.5 + 5.4 = 8.3 $\n",
        "  - Second column: $ 4.0 \\cdot 0.2 + 5.0 \\cdot 0.6 + 6.0 \\cdot 1.0 = 0.8 + 3.0 + 6.0 = 9.8 $\n",
        "  - Third column: $ 4.0 \\cdot 0.3 + 5.0 \\cdot 0.7 + 6.0 \\cdot 1.1 = 1.2 + 3.5 + 6.6 = 11.3 $\n",
        "  - Fourth column: $ 4.0 \\cdot 0.4 + 5.0 \\cdot 0.8 + 6.0 \\cdot 1.2 = 1.6 + 4.0 + 7.2 = 12.8 $\n",
        "\n",
        "Thus, we have:\n",
        "\n",
        "$\n",
        "\\mathbf{X} \\mathbf{W} = \\begin{bmatrix} 3.8 & 4.4 & 5.0 & 5.6 \\\\ 8.3 & 9.8 & 11.3 & 12.8 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "\n",
        "\n",
        "### Step 2: Adding Biases\n",
        "\n",
        "$\\mathbf{b}  =\\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ 0.4 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "Since $ \\mathbf{b} $ is a column vector, it will be added to each row of $ \\mathbf{X} \\mathbf{W} $:\n",
        "\n",
        "- For the first row:\n",
        "  - $ 3.8 + 0.1 = 3.9 $\n",
        "  - $ 4.4 + 0.2 = 4.6 $\n",
        "  - $ 5.0 + 0.3 = 5.3 $\n",
        "  - $ 5.6 + 0.4 = 6.0 $\n",
        "\n",
        "- For the second row:\n",
        "  - $ 8.3 + 0.1 = 8.4 $\n",
        "  - $ 9.8 + 0.2 = 10.0 $\n",
        "  - $ 11.3 + 0.3 = 11.6 $\n",
        "  - $ 12.8 + 0.4 = 13.2 $\n",
        "\n",
        "Thus\n",
        "\n",
        "$\n",
        "\\mathbf{Z} = \\begin{bmatrix} 3.9 & 4.6 & 5.3 & 6.0 \\\\ 8.4 & 10.0 & 11.6 & 13.2 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "### Step 3: Activation Function (ReLU)\n",
        "\n",
        "Next, apply the ReLU activation function $ \\sigma(\\mathbf{Z}) = \\max(0, \\mathbf{Z}) $:\n",
        "\n",
        "$\n",
        "\\mathbf{A} = \\begin{bmatrix} \\max(0, 3.9) & \\max(0, 4.6) & \\max(0, 5.3) & \\max(0, 6.0) \\\\ \\max(0, 8.4) & \\max(0, 10.0) & \\max(0, 11.6) & \\max(0, 13.2) \\end{bmatrix}\n",
        "$\n",
        "\n",
        "In this example, since all values are positive, so **Output of the Dense Layer**:\n",
        "\n",
        "$\n",
        "\\mathbf{A} = \\begin{bmatrix} 3.9 & 4.6 & 5.3 & 6.0 \\\\ 8.4 & 10.0 & 11.6 & 13.2 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "This output $ \\mathbf{A} $ from the Dense() layer is next passed to the Batch Normalization() layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "WfW_v78NlDFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Batch Normalization calculations\n",
        "\n",
        "\n",
        "### Step 1: Mean and Variance Calculation\n",
        "\n",
        "Given the output $\\mathbf{A} $:\n",
        "\n",
        "$\n",
        "\\mathbf{A} = \\begin{bmatrix} 3.9 & 4.6 & 5.3 & 6.0 \\\\ 8.4 & 10.0 & 11.6 & 13.2 \\end{bmatrix}\n",
        "$\n",
        "\n",
        " **Mean $\\mu $**:\n",
        "\n",
        "\n",
        "   $\n",
        "   \\mu = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{A}_i\n",
        "   $\n",
        "\n",
        "   Here, $n = 2 $ (the batch size), and we compute the mean for each feature across the batch:\n",
        "\n",
        "   - For the first feature:\n",
        "     $\n",
        "     \\mu_1 = \\frac{3.9 + 8.4}{2} = \\frac{12.3}{2} = 6.15\n",
        "     $\n",
        "   - For the second feature:\n",
        "     $\n",
        "     \\mu_2 = \\frac{4.6 + 10.0}{2} = \\frac{14.6}{2} = 7.3\n",
        "     $\n",
        "   - For the third feature:\n",
        "     $\n",
        "     \\mu_3 = \\frac{5.3 + 11.6}{2} = \\frac{16.9}{2} = 8.45\n",
        "     $\n",
        "   - For the fourth feature:\n",
        "     $\n",
        "     \\mu_4 = \\frac{6.0 + 13.2}{2} = \\frac{19.2}{2} = 9.6\n",
        "     $\n",
        "\n",
        "   Thus, the mean vector $\\mu $ is:\n",
        "\n",
        "   $\n",
        "   \\mu = \\begin{bmatrix} 6.15 \\\\ 7.3 \\\\ 8.45 \\\\ 9.6 \\end{bmatrix}\n",
        "   $\n",
        "\n",
        "**Variance $\\sigma^2 $**:\n",
        "   \n",
        "\n",
        "   $\n",
        "   \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{A}_i - \\mu)^2\n",
        "   $\n",
        "\n",
        "   We compute the variance for each feature:\n",
        "\n",
        "   - For the first feature:\n",
        "     $\n",
        "     \\sigma_1^2 = \\frac{(3.9 - 6.15)^2 + (8.4 - 6.15)^2}{2} = \\frac{(-2.25)^2 + (2.25)^2}{2} = \\frac{5.0625 + 5.0625}{2} = \\frac{10.125}{2} = 5.0625\n",
        "     $\n",
        "   - For the second feature:\n",
        "     $\n",
        "     \\sigma_2^2 = \\frac{(4.6 - 7.3)^2 + (10.0 - 7.3)^2}{2} = \\frac{(-2.7)^2 + (2.7)^2}{2} = \\frac{7.29 + 7.29}{2} = \\frac{14.58}{2} = 7.29\n",
        "     $\n",
        "   - For the third feature:\n",
        "     $\n",
        "     \\sigma_3^2 = \\frac{(5.3 - 8.45)^2 + (11.6 - 8.45)^2}{2} = \\frac{(-3.15)^2 + (3.15)^2}{2} = \\frac{9.9225 + 9.9225}{2} = \\frac{19.845}{2} = 9.9225\n",
        "     $\n",
        "   - For the fourth feature:\n",
        "     $\n",
        "     \\sigma_4^2 = \\frac{(6.0 - 9.6)^2 + (13.2 - 9.6)^2}{2} = \\frac{(-3.6)^2 + (3.6)^2}{2} = \\frac{12.96 + 12.96}{2} = \\frac{25.92}{2} = 12.96\n",
        "     $\n",
        "\n",
        "   Thus, the variance vector $\\sigma^2 $ is:\n",
        "\n",
        "   $\n",
        "   \\sigma^2 = \\begin{bmatrix} 5.0625 \\\\ 7.29 \\\\ 9.9225 \\\\ 12.96 \\end{bmatrix}\n",
        "   $\n",
        "\n"
      ],
      "metadata": {
        "id": "wY9rrgrlqzPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Compute the normalized output $ \\hat{\\mathbf{A}} $ using the mean $ \\mu $ and variance $ \\sigma^2 $ calculated previously:\n",
        "\n",
        "$\n",
        "\\hat{\\mathbf{A}} = \\frac{\\mathbf{A} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
        "$\n",
        "\n",
        "where $ \\epsilon $ is a small constant added for numerical stability (commonly set to $ 1 \\times 10^{-5} $ or similar). For this example, let's use $ \\epsilon = 1 \\times 10^{-5} $.\n",
        "\n",
        "**Calculate $ \\sqrt{\\sigma^2 + \\epsilon} $**:\n",
        "\n",
        "   $\n",
        "   \\sqrt{\\sigma^2 + \\epsilon} = \\begin{bmatrix} \\sqrt{5.0625 + 1 \\times 10^{-5}} \\\\ \\sqrt{7.29 + 1 \\times 10^{-5}} \\\\ \\sqrt{9.9225 + 1 \\times 10^{-5}} \\\\ \\sqrt{12.96 + 1 \\times 10^{-5}} \\end{bmatrix}\n",
        "   $\n",
        "\n",
        "   Approximating the square roots:\n",
        "\n",
        "   - For the first feature:\n",
        "     $\n",
        "     \\sqrt{5.0625 + 1 \\times 10^{-5}} \\approx \\sqrt{5.0625} \\approx 2.25\n",
        "     $\n",
        "   - For the second feature:\n",
        "     $\n",
        "     \\sqrt{7.29 + 1 \\times 10^{-5}} \\approx \\sqrt{7.29} \\approx 2.7\n",
        "     $\n",
        "   - For the third feature:\n",
        "     $\n",
        "     \\sqrt{9.9225 + 1 \\times 10^{-5}} \\approx \\sqrt{9.9225} \\approx 3.15\n",
        "     $\n",
        "   - For the fourth feature:\n",
        "     $\n",
        "     \\sqrt{12.96 + 1 \\times 10^{-5}} \\approx \\sqrt{12.96} \\approx 3.6\n",
        "     $\n",
        "\n",
        "   Thus, we have:\n",
        "\n",
        "   $\n",
        "   \\sqrt{\\sigma^2 + \\epsilon} \\approx \\begin{bmatrix} 2.25 \\\\ 2.7 \\\\ 3.15 \\\\ 3.6 \\end{bmatrix}\n",
        "   $\n",
        "\n",
        "**Calculate $ \\hat{\\mathbf{A}} $**:\n",
        "\n",
        "Now we can compute $ \\hat{\\mathbf{A}} $:\n",
        "\n",
        "$\n",
        "\\hat{\\mathbf{A}} = \\frac{\\mathbf{A} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} = \\begin{bmatrix} 3.9 & 4.6 & 5.3 & 6.0 \\\\ 8.4 & 10.0 & 11.6 & 13.2 \\end{bmatrix} - \\begin{bmatrix} 6.15 \\\\ 7.3 \\\\ 8.45 \\\\ 9.6 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "Calculating $ \\mathbf{A} - \\mu $:\n",
        "\n",
        "- For the first row:\n",
        "  - $ 3.9 - 6.15 = -2.25 $\n",
        "  - $ 4.6 - 7.3 = -2.7 $\n",
        "  - $ 5.3 - 8.45 = -3.15 $\n",
        "  - $ 6.0 - 9.6 = -3.6 $\n",
        "\n",
        "- For the second row:\n",
        "  - $ 8.4 - 6.15 = 2.25 $\n",
        "  - $ 10.0 - 7.3 = 2.7 $\n",
        "  - $ 11.6 - 8.45 = 3.15 $\n",
        "  - $ 13.2 - 9.6 = 3.6 $\n",
        "\n",
        "Thus,\n",
        "\n",
        "$\n",
        "\\mathbf{A} - \\mu = \\begin{bmatrix} -2.25 & -2.7 & -3.15 & -3.6 \\\\ 2.25 & 2.7 & 3.15 & 3.6 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "**Next, divide  by $ \\sqrt{\\sigma^2 + \\epsilon} $**:\n",
        "\n",
        "$\n",
        "\\hat{\\mathbf{A}} = \\begin{bmatrix} \\frac{-2.25}{2.25} & \\frac{-2.7}{2.7} & \\frac{-3.15}{3.15} & \\frac{-3.6}{3.6} \\\\ \\frac{2.25}{2.25} & \\frac{2.7}{2.7} & \\frac{3.15}{3.15} & \\frac{3.6}{3.6} \\end{bmatrix}\n",
        "$\n",
        "\n",
        "$ = \\begin{bmatrix} -1 & -1 & -1 & -1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "\n",
        "\n",
        "### Step 3: Scaling and Shifting:\n",
        "\n",
        "\n",
        "$\n",
        "\\mathbf{Y} = \\gamma \\hat{\\mathbf{A}} + \\beta\n",
        "$\n",
        "\n",
        "Where $ \\gamma $ and $ \\beta $ are learnable parameters. For this example, let's assume:\n",
        "\n",
        "- $ \\gamma = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} $ (scale factor)\n",
        "- $ \\beta = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} $ (shift factor)\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$\n",
        "\\mathbf{Y} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\begin{bmatrix} -1 & -1 & -1 & -1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "Then, calculating $ \\gamma \\hat{\\mathbf{A}} $:\n",
        "\n",
        "$\n",
        "\\gamma \\hat{\\mathbf{A}} = \\begin{bmatrix} -1 & -1 & -1 & -1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "\n",
        "Finally, adding the shift $ \\beta $:\n",
        "\n",
        "$\n",
        "\\mathbf{Y} = \\begin{bmatrix} -1 & -1 & -1 & -1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -1 & -1 & -1 & -1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}\n",
        "$\n",
        "\n",
        "Thus,\n",
        "\n",
        "**Final Output**:\n",
        "  $\n",
        "   \\mathbf{Y} = \\begin{bmatrix} -1 & -1 & -1 & -1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}\n",
        "  $\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UTzezRdJmRwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of tensor operations in a neural network using TensorFlow Playground."
      ],
      "metadata": {
        "id": "j3F7XZk_zUZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "To visualize a classification problem similar to the Iris dataset, simulate a multi-class classification problem in TensorFlow Playground. Follow these steps:\n",
        "\n",
        "1. **Go to TensorFlow Playground**: Open your web browser and navigate to [TensorFlow Playground](http://playground.tensorflow.org).\n",
        "\n",
        "2. **Select a Similar Dataset**:\n",
        "   - In the top left corner, you will see a dropdown menu for datasets. Choose one of the following datasets that resemble a multi-class classification problem:\n",
        "     - **Spiral**\n",
        "     - **Circles**\n",
        "     - **Moons**\n",
        "   - These datasets will allow you to visualize how a neural network can learn to classify different classes.\n",
        "\n",
        "3. **Configure the Neural Network**:\n",
        "   - **Add Layers**: Click on the \"+\" button to add a Dense layer. Start with one hidden layer and set the number of neurons to either 4 or 8.\n",
        "   - **Activation Function**: For the hidden layer, select the \"ReLU\" activation function from the dropdown menu.\n",
        "   - **Output Layer**: Ensure that the output layer has multiple neurons. For example, if you are using the Iris dataset as a reference, set the output layer to have 3 neurons to represent the three species.\n",
        "\n",
        "4. **Adjust Hyperparameters**:\n",
        "   - Set the **learning rate** to a suitable value (e.g., 0.01 or 0.1).\n",
        "   - Adjust the **regularization** settings if needed (you can start with no regularization).\n",
        "   - Set the **number of epochs** to a higher value, such as 200 or 300, to allow the model to train longer.\n",
        "\n",
        "5. **Train the Model**:\n",
        "   - Click the \"Run\" button to start training the model.\n",
        "   - Observe how the model learns to classify the data points. You can visualize the decision boundaries and see how well the model performs on the selected dataset.\n",
        "\n",
        "Enjoy experimenting with different configurations and observing how the neural network learns!\n"
      ],
      "metadata": {
        "id": "LHJ1bi9bzx5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Conclusion\n",
        "\n",
        "In this notebook, I've explored neural network architecture using TensorFlow.\n",
        "\n",
        "Understanding the mathematics underlying neural network model architecture is crucial for effectively building and training neural networks.\n"
      ],
      "metadata": {
        "id": "skYb14iM8rDZ"
      }
    }
  ]
}